---
title: 'Lecture XI: Inference with Three or More Sample Means'
header-includes:
  \usepackage{multirow}
output: pdf_document
---

# X. Inference with Three or More Sample Means

1. Logic of ANOVA
2. Computation of ANOVA
3. Measures of association

## Logic of ANOVA

* We have just encountered research problems in which we want to compare two sample means in order to make inferences about the two groups in the larger population. Now suppose that we want to compare means from three or more samples. In this case, we have a categorical independent with three or more categories, and a continuous dependent variable. When we have three or more sample means, the question we are interested in answering is this: Do the differences that we observe among the sample means indicate that there are significant differences across groups in the population? This is just a generalization of the two-sample case. There are several examples of such problems: sentence length as a function of offense type (violent, property, drug, other), fear of crime as a function of residential location (urban, suburban, rural), and offending as a function of race/ethnicity (white, black, Hispanic, Asian, other). 

* One way to deal with this type of data is to carry out a series of two-sample hypothesis tests. An obvious disadvantage to this strategy is that the more groups that we have, the greater number of pairwise t-tests that we have to conduct. For example, say that we have an independent variable with three categories. This means that we would have to conduct (using the combination rule) $3!/2!(3 - 2)! = 3$ separate hypothesis tests. With four groups there are 6 hypothesis tests, with five groups there are 10 tests, with six groups there are 15 tests, with seven groups there are 21 tests, and so on. As you can see, the number of hypothesis tests quickly becomes unmanageable. 

* A more substantive disadvantage to conducting multiple hypothesis tests with three or more samples is that, since each of the tests is conducted on the same data, they are not independent of one another. The practical implication of this is that over multiple tests, the probability of committing a type I error (i.e., the probability of falsely rejecting the null hypothesis) on any given test is greater than ??. As an example, suppose that we have three groups. We can determine using the binomial formula that the probability that we falsely reject the null hypothesis on at least one hypothesis test using a 5% criterion is $p(r > 0 | n = 3) = 1 -  \text{C}^{3}_{0}(.05)^{0}(.95)^{3} = .143$. If we increase the number of hypothesis tests, we find that $p(r > 0 | n = 4) = .185$, $p(r > 0 | n = 5) = .226$, $p(r > 0 | n = 6) = .265$, $p(r > 0 | n = 7) = .302$, and so on. Clearly, the probability that we would falsely reject in at least one hypothesis test becomes considerably greater than $\alpha$ = .05. 

* As you already know, we want a statistical test that will help us decide whether these observed differences are the result of sampling variation or (presumably) real differences in sentence length. Analysis of variance (ANOVA) is useful for determining the extent to which there are statistically significant differences between three or more sample means. We refer to ANOVA as a global test, which means that it tests the joint significance of several sample means, rather than differences among specific pairs. The advantage to conducting a global test is that the probability of committing a type I error is constant.

* Why is variance so important, when we are actually interested in comparing means? With ANOVA, we speak of two different kinds of variability: variability within and between groups. Let's consider each separately. Variability *within* groups refers to how tightly clustered individual scores are from their group mean. When this variability is small, each of the cases within a group cluster tightly around their respective group means, indicating that more of the cases are similar within a particular group than are different. Variability *between* groups (or across groups) refers to how tightly clustered the sample means are from each other, or from what we refer to as a *grand mean*. When the variability between groups is large, the group means are only loosely clustered around the grand mean, indicating that the group means are more different than they are similar. 

* Now consider the ratio of between-group variability to within-group variability, and its implication for statistical inference. We refer to this as an *F*-ratio. When there is more variability within groups than between groups, the *F*-ratio will be less than one. This means that there is a great deal of overlap among the group distributions, and thus there is no relationship between group membership and the dependent variable. When there is more variability between groups than within groups, the *F*-ratio will be greater than one. This means that there is little or no overlap among the groups, and thus group membership is associated with the dependent variable. 

* When we use ANOVA, we rely on a new probability distribution: the *F*-distribution. An *F*-ratio is simply the ratio of the variability between groups to the variability within groups. When we have a large *F*-ratio (i.e., one that is significantly greater than one), we will be led to reject the null hypothesis of no association between group membership and the outcome of interest. 

\newpage

## Computation of ANOVA

* In ANOVA terminology, we are interested in a measure of variability called the sum of squared deviations about the mean, or simply the *sum of squares*. You might remember that the sum of squares is simply the numerator of the variance formula. A quick tutorial on notation: $n$ refers to the sample size of a particular group, whereas $N$ refers to the number of cases across all groups. There are three different sums of squares that we want to know in ANOVA. 
    - **Total sum of squares**: This is the sum of the squared deviations of each case around the *grand mean*. $$SS_{T}=\sum{(\text{x}_{ik}-\overline{\text{x}}_{G})^{2}}=\sum{\text{x}^{2}_{i}-N\overline{\text{x}}^{2}_{G}}$$
    - **Between-groups sum of squares**: This is the sum of the squared deviations of each group mean around the *grand mean*. $$SS_{B}=\sum{n_{k}(\overline{\text{x}}_{k}-\overline{\text{x}}_{G})^{2}}=\sum{n_{k}\overline{\text{x}}^{2}_{k}-N\overline{\text{x}}_{G}^{2}}$$
    - **Within-groups sum of squares**. This is the sum of squared deviations of each case around its respective group mean. $$SS_{W}=\sum{(\text{x}_{ik}-\overline{\text{x}}_{k})^{2}}=\sum{\text{x}^{2}_{i}}-\sum{n_{k}\overline{\text{x}}_{k}^{2}}=SS_{T}-SS_{B}$$
    - The relationship among these three different sums of squares is straightforward. $$SS_{T}=SS_{B}+SS_{W}$$
* In addition to the sums of squares, we will also need to know our degrees of freedome:
    - **Total degrees of freedom**: $df_{T}=N-1$
    - **Between-group degrees of freedom**: $df_{B}=k-1$
    - **Within-group degrees of freedom**: $df_{W}=N-k$
* Using this information, we can partition the variance into the total variance, the between-group variance, and the within-group variance. The variance is computed as the sum of squares divided by the respective degrees of freedom. The *F*-statistic is calculated as the ratio of the between-group variance to the within-group variance. 

* Let's carry out a full hypothesis test with the data from a sentence length example. Suppose that we have a sample of 40 offenders that have committed one of four types of offenses. Our independent variable is offense type, and our dependent variable is sentence length in months.
    - **Step 1: State hypotheses** - With ANOVA problems, the alternative hypothesis is always $\text{H}_{1}: \mu_{1} \ne \mu_{2} \ne \mu_{3} \ne \mu_{4}$, and the null hypothesis is: $\text{H}_{0}: \mu_{1}=\mu_{2}=\mu_{3}=\mu_{4}$. We make the assumption under the null hypothesis that the population means are equivalent. Under the alternative hypothesis, we make the assumption that at least one population mean is significantly different from at least one other population mean. ANOVA is what we call a *global test*, in that it can tell us if there are significant differences in the means, but it cannot tell us exactly which means are significantly different. 
    - **Step 2: Obtain a probability distribution** - In order to do an ANOVA, we have to resort to the *F*-distribution. The *F*-distribution is one-tailed, because we square the deviations. The shape of the *F*-distribution is defined by two sets of degrees of freedom. The *df* in the numerator is $df_{B}=k-1=3$ (top row of the *F*-table) and the *df* in the denominator is $df_{W}=N-k=36$ (far left column of the *F*-table). 
    - **Step 3: Make decision rules** - Let's use $\alpha$=.01. The critical value of *F* is 4.31 (round up to 40 within-group degrees of freedom), thus we will reject the null hypothesis if *F*>4.31.
    
    \newpage
    - **Step 4: Calculate the test statistic**.
    \begin{table}[h]
    \centering
    \begin{tabular}{cccccccc}
    \hline
    \multicolumn{2}{c}{Violent} & \multicolumn{2}{c}{Property}  & \multicolumn{2}{c}{Drug}  & \multicolumn{2}{c}{Other} \\
    $\text{x}_{1}$  & $\text{x}_{1}^{2}$  & $\text{x}_{2}$  & $\text{x}_{2}^{2}$  & $\text{x}_{3}$  & $\text{x}_{3}^{2}$  & $\text{x}_{4}$  & $\text{x}_{4}^{2}$  \\ \hline
    6 & `r 6^2` & 4 & `r 4^2` & 6 & `r 6^2` & 1 & `r 1^2` \\
    18 & `r 18^2` & 6 & `r 6^2` & 3 & `r 3^2` & 3 & `r 3^2` \\
    20 & `r 20^2` & 3 & `r 3^2` & 3 & `r 3^2` & 1 & `r 1^2` \\
    15 & `r 15^2` & 10 & `r 10^2` & 4 & `r 4^2` & 1 & `r 1^2` \\
    20 & `r 20^2` & 12 & `r 12^2` & 6 & `r 6^2` & 6 & `r 6^2` \\
    30 & `r 30^2` & 8 & `r 8^2` & 9 & `r 9^2` & 9 & `r 9^2` \\
    25 & `r 25^2` & 6 & `r 6^2` & 10 & `r 10^2` & 3 & `r 3^2` \\
    12 & `r 12^2` & 10 & `r 10^2` & 3 & `r 3^2` & 6 & `r 6^2` \\
    24 & `r 24^2` & 15 & `r 15^2` & 3 & `r 3^2` & 4 & `r 4^2` \\ \hline
    190 & 4030  & 82  & 794 & 49  & 309 & 36  & 194 \\
    \hline
    \end{tabular}
    \end{table}
    - There are several pieces of information that we need to obtain to calculate the *F*-statistic. First, we need to compute the group means as well as the grand mean. 
        - $\overline{x}_{1}=\sum{\text{x}_{i1}/n_{1}}=190/10=`r 190/10`$
        \vspace{2pt}
        - $\overline{x}_{2}=\sum{\text{x}_{i2}/n_{2}}=82/10=`r 82/10`$
        \vspace{2pt}
        - $\overline{x}_{3}=\sum{\text{x}_{i3}/n_{3}}=49/10=`r 49/10`$
        \vspace{2pt}
        - $\overline{x}_{4}=\sum{\text{x}_{i4}/n_{4}}=36/10=`r 36/10`$
        \vspace{2pt}
        - $\displaystyle \overline{x}_{G}=\sum{\text{x}_{i}}/N=\frac{190+82+49+36}{10+10+10+10}=`r (190+82+49+36)/(10+10+10+10)`$
    - Second, we need to compute the sums of squares. This step only requires us to find the total sum of squares and between-group sum of squares, which we can use to solve for the within-group sum of squares. 
        - $SS_{T}=\sum{\text{x}^{2}_{i}-N\overline{\text{x}}^{2}_{G}}=(4030+794+309+194)-40(8.9)^{2}=5327-3168.40=`r (4030+794+309+194)-(40*(8.9^2))`$
        \vspace{2pt}
        - $SS_{B}=\sum{\text{n}_{k}\overline{\text{x}}_{k}^{2}}-N\overline{\text{x}}_{G}^{2}=10(19.0)^{2}+10(8.2)^{2}+10(4.9)^{2}+10(3.6)^{2}-40(8.9)^{2}=`r (10*(19.0^2))+(10*(8.2^2))+(10*(4.9^2))+(10*(3.6^2))-(40*(8.9^2))`$
        \vspace{2pt}
        - $SS_{W}=\sum{\text{x}^{2}_{i}}-\sum{\text{n}_{k}\overline{\text{x}}^{2}_{k}}=SS_{T}-SS_{B}=2158.6-1483.7=`r 2158.6-1483.7`$
    - Third, we use this information to calculate the $F$-statistic. It is convenient to put ANOVA data into the form of a table. 
    \begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
    \hline
    Source          & SS      & $df$            & $MS=SS/df$                              & $F=MS_{B}/MS_{W}$ \\ \hline
    Between groups  & 1483.7  & $k-1=`r 4-1`$   & `r format(1483.7/3,nsmall=2,digits=2)`  & \multirow{3}{*}{$\displaystyle \frac{494.57}{18.75}=`r format(494.57/18.75,nsmall=2,digits=2)`$} \\
    Within groups   & 674.9   & $N-k=`r 40-4`$  & `r format(674.9/36,nsmall=2,digits=2)`  & \\
    Total           & 2158.6  & $N-1=`r 40-1`$  & `r format(2158.6/39,nsmall=2,digits=2)` & \\ \hline
    \end{tabular}
    \end{table}
    - **Step 5: Make a decision about the null hypothesis** - Make a decision about the null hypothesis. Since $F$ = 26.39, we reject the null hypothesis, and conclude that offense type is significantly associated with sentence length. Recall, however, that an $F$-test, since it is global, cannot tell us anything more substantive than this. We know that there are significant differences, but without conducting further tests (which we call post-hoc tests) we are unable to draw any further conclusions.
    
\newpage

Let's consider another example

* You collect data on fear of crime from a sample of 30 individuals, divided equally among urban, suburban, and rural areas. The research question is this: Is area of residence related to fear of crime?
\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\hline
\multicolumn{2}{c}{Urban} & \multicolumn{2}{c}{Suburban}  & \multicolumn{2}{c}{Rural} \\
$\text{x}_{U}$  & $\text{x}^{2}_{U}$  & $\text{x}_{S}$  & $\text{x}_{S}^{2}$  & $\text{x}_{R}$  & $\text{x}_{R}^{2}$  \\ \hline
22  & `r 22^2`  & 23  & `r 23^2`  & 19  & `r 19^2`  \\
29  & `r 29^2`  & 22  & `r 22^2`  & 24  & `r 24^2`  \\
31  & `r 31^2`  & 26  & `r 26^2`  & 24  & `r 24^2`  \\
28  & `r 28^2`  & 25  & `r 25^2`  & 19  & `r 19^2`  \\
30  & `r 30^2`  & 24  & `r 24^2`  & 20  & `r 20^2`  \\
32  & `r 32^2`  & 25  & `r 25^2`  & 24  & `r 24^2`  \\
32  & `r 32^2`  & 24  & `r 24^2`  & 21  & `r 21^2`  \\
31  & `r 31^2`  & 24  & `r 24^2`  & 17  & `r 17^2`  \\
28  & `r 28^2`  & 27  & `r 27^2`  & 23  & `r 23^2`  \\
30  & `r 30^2`  & 23  & `r 23^2`  & 19  & `r 19^2`  \\ \hline
`r 22+29+31+28+30+32+32+31+28+30` & `r 484+841+961+784+900+1024+1024+961+784+900` & `r 23+22+26+25+24+25+24+24+27+23` & `r 529+484+676+625+576+625+576+576+729+529` &
`r 19+24+24+19+20+24+21+17+23+19` & `r 361+576+576+361+400+576+441+289+529+361` \\ \hline
\end{tabular}
\end{table}
    - **Step 1: State hypotheses** - $\text{H}_{1}: \mu_{U} \ne \mu_{S} \ne \mu_{R}$; $\text{H}_{0}: \mu_{U} = \mu_{S} = \mu_{R}$
    - **Step 2: Obtain a probability distribution** - $F$-distribution, $df_{B}=3-1=`r 3-1`$, $df_{W}=30-3=`r 30-3`$
    - **Step 3: Make decision rules** - $\alpha=.05$' $F_{crit}=$; reject $\text{H}_{0}$ if $F>$
    - **Step 4: Calculate the test statistic** - 
        - $\overline{x}_{1}=\sum{\text{x}_{i1}/n_{1}}=293/10=`r 293/10`$
        \vspace{2pt}
        - $\overline{x}_{2}=\sum{\text{x}_{i2}/n_{2}}=243/10=`r 243/10`$
        \vspace{2pt}
        - $\overline{x}_{3}=\sum{\text{x}_{i3}/n_{3}}=210/10=`r format(210/10,nsmall=1,digits=1)`$
        \vspace{2pt}
        - $\displaystyle \overline{x}_{G}=\sum{\text{x}_{i}}/N=\frac{293+243+210}{10+10+10}=`r format((293+243+210)/(10+10+10),nsmall=2,digits=2)`$
        - $SS_{T}=\sum{\text{x}^{2}_{i}-N\overline{\text{x}}^{2}_{G}}=(8663+5925+4470)-30(24.87)^{2}=`r (8663+5925+4470)-(30*(24.87^2))`$
        \vspace{2pt}
        - $SS_{B}=\sum{\text{n}_{k}\overline{\text{x}}_{k}^{2}}-N\overline{\text{x}}_{G}^{2}=10(29.3)^{2}+10(24.3)^{2}+10(21.0)^{2}-30(24.87)^{2}=`r (10*(29.3^2))+(10*(24.3^2))+(10*(21.0^2))-(30*(24.87^2))`$
        \vspace{2pt}
        - $SS_{W}=\sum{\text{x}^{2}_{i}}-\sum{\text{n}_{k}\overline{\text{x}}^{2}_{k}}=SS_{T}-SS_{B}=502.493-344.293=`r 502.493-344.293`$
    \begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
    \hline
    Source          & SS        & $df$            & $MS=SS/df$                              & $F=MS_{B}/MS_{W}$ \\ \hline
    Between groups  & 344.293   & $k-1=`r 3-1`$   & `r format(344.293/2,nsmall=3,digits=3)`  & \multirow{3}{*}{$\displaystyle \frac{172.147}{5.859}=`r format(172.147/5.859,nsmall=2,digits=2)`$} \\
    Within groups   & 158.2     & $N-k=`r 30-3`$  & `r format(158.2/27,nsmall=3,digits=3)`  & \\
    Total           & 502.493   & $N-1=`r 30-1`$  & `r format(502.493/29,nsmall=3,digits=3)` & \\ \hline
    \end{tabular}
    \end{table}

    - **Step 5: Make a decision about the null hypothesis** - Reject $H_{0}$, conclude that area of residence is related to fear of crime.  
    
\newpage

## Measures of Association

*	While ANOVA can tell us whether there is a significant relationship between two variables, it cannot tell us anything about the strength of the relationship. We can, however, utilize what we know about variance to compute a measure of the strength of the association between the variables. We have available to us two measures of association called eta-square and epsilon-square, which are computed, respectively, as $$\eta^{2}=\frac{SS_{B}}{SS_{T}}$$ $$\epsilon^{2}=1-\frac{MS_{W}}{MS_{T}}=1-\frac{SS_{W}/df_{W}}{SS_{T}/df_{T}}$$
Both have an **explained variance** interpretation. They tell us what proportion of the total variance in the dependent variable is explained by the independent variable. Epsilon-square ($\epsilon^{2}$) is a more conservative estimate, since it takes into account degrees of freedom. 

* Let's consider our sentence length example. We compute the measures of association as $$\eta^{2}=\frac{SS_{B}}{SS_{T}}=\frac{1483.7}{2158.6}=`r format(1483.7/2158.6,nsmall=3,digits=3)`$$ $$\epsilon^{2}=1-\frac{MS_{W}}{MS_{T}}=1-\frac{SS_{W}/df_{W}}{SS_{T}/df_{T}}=1-\frac{18.75}{55.35}=`r format(1-(18.75/55.35),nsmall=3,digits=3)`$$ We interpret these by saying that between 66.1% and 68.7% of the variance in sentence length is explained by offense type. Anything above 50% is generally a strong association. 

* Let's compute these for the fear of crime example. 

$$\eta^{2}=\frac{SS_{B}}{SS_{T}}=\frac{344.293}{502.493}=`r format(344.293/502.493,nsmall=3,digits=3)`$$
$$\epsilon^{2}=1-\frac{MS_{W}}{MS_{T}}=1-\frac{SS_{W}/df_{W}}{SS_{T}/df_{T}}=1-\frac{5.859}{17.327}=`r format(1-(5.859/17.327),nsmall=3,digits=3)`$$

* Between 66.2% and 68.5% of the variance in fear of crime is explained by residential location. 
        